<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Most Imaginative New Applications of 2024 | DARKNAVY</title>
<meta name=keywords content><meta name=description content="
2023 was the dawn of generative AI and large language models, which output content in unprecedented ways.
In 2024, a large number of AI agents emerged, expanding the capabilities of LLM, driving more widespread tool usage, and extending their application to more fields.
For security researchers, how to leverage AI to improve work efficiency, and even drive AI to think, analyze, and find vulnerabilities like humans, has become a key topic."><meta name=author content="DARKNAVY"><link rel=canonical href=https://www.darknavy.org/darknavy_insight/the_most_imaginative_new_applications_of_2024/><link crossorigin=anonymous href=/assets/css/stylesheet.86b34fc8709322911fa9e50a0b6fbe57a1a032cdd591c9f647724be25533842d.css integrity="sha256-hrNPyHCTIpEfqeUKC2++V6GgMs3Vkcn2R3JL4lUzhC0=" rel="preload stylesheet" as=style><link rel=icon href=https://www.darknavy.org/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.darknavy.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.darknavy.org/favicon-32x32.png><link rel=apple-touch-icon href=https://www.darknavy.org/apple-touch-icon.png><link rel=mask-icon href=https://www.darknavy.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.darknavy.org/darknavy_insight/the_most_imaginative_new_applications_of_2024/><link rel=alternate hreflang=zh href=https://www.darknavy.org/zh/darknavy_insight/the_most_imaginative_new_applications_of_2024/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-LR4ZN1LSPS"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LR4ZN1LSPS")}</script><meta property="og:url" content="https://www.darknavy.org/darknavy_insight/the_most_imaginative_new_applications_of_2024/"><meta property="og:site_name" content="DARKNAVY"><meta property="og:title" content="The Most Imaginative New Applications of 2024"><meta property="og:description" content="
2023 was the dawn of generative AI and large language models, which output content in unprecedented ways.
In 2024, a large number of AI agents emerged, expanding the capabilities of LLM, driving more widespread tool usage, and extending their application to more fields.
For security researchers, how to leverage AI to improve work efficiency, and even drive AI to think, analyze, and find vulnerabilities like humans, has become a key topic."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="darknavy_insight"><meta property="article:published_time" content="2025-02-10T11:25:50+08:00"><meta property="article:modified_time" content="2025-02-10T11:25:50+08:00"><meta property="og:image" content="https://www.darknavy.org/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/74e16a34-d20c-4282-99af-4f93482ad22e.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.darknavy.org/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/74e16a34-d20c-4282-99af-4f93482ad22e.png"><meta name=twitter:title content="The Most Imaginative New Applications of 2024"><meta name=twitter:description content="
2023 was the dawn of generative AI and large language models, which output content in unprecedented ways.
In 2024, a large number of AI agents emerged, expanding the capabilities of LLM, driving more widespread tool usage, and extending their application to more fields.
For security researchers, how to leverage AI to improve work efficiency, and even drive AI to think, analyze, and find vulnerabilities like humans, has become a key topic."><meta name=twitter:site content="@DarkNavyOrg"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"DARKNAVY INSIGHT","item":"https://www.darknavy.org/darknavy_insight/"},{"@type":"ListItem","position":2,"name":"The Most Imaginative New Applications of 2024","item":"https://www.darknavy.org/darknavy_insight/the_most_imaginative_new_applications_of_2024/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Most Imaginative New Applications of 2024","name":"The Most Imaginative New Applications of 2024","description":"\n2023 was the dawn of generative AI and large language models, which output content in unprecedented ways.\nIn 2024, a large number of AI agents emerged, expanding the capabilities of LLM, driving more widespread tool usage, and extending their application to more fields.\nFor security researchers, how to leverage AI to improve work efficiency, and even drive AI to think, analyze, and find vulnerabilities like humans, has become a key topic.\n","keywords":[],"articleBody":"\n2023 was the dawn of generative AI and large language models, which output content in unprecedented ways.\nIn 2024, a large number of AI agents emerged, expanding the capabilities of LLM, driving more widespread tool usage, and extending their application to more fields.\nFor security researchers, how to leverage AI to improve work efficiency, and even drive AI to think, analyze, and find vulnerabilities like humans, has become a key topic.\nWill we embrace AI first, or will AI replace us? When will this day arrive?\nThe following is the third article of the “DARKNAVY INSIGHT | 2024 Annual Security Report”.\nWith the emergence of generative AI, its impressive code understanding abilities undoubtedly show its potential to revolutionize the field of security research.\nSince 2023, security researchers have begun trying to use the knowledge base and content generation abilities of LLMs to improve the efficiency of various stages in security research:\nAsking LLMs questions to help security researchers quickly understand the functionality of code, using LLMs to quickly generate test code, integrating LLMs into IDEs to provide coding security suggestions, etc.\nWe also saw the emergence of a series of LLM-based tools:\nUsing LLMs as a security research knowledge base can effectively improve the efficiency of solving unfamiliar domain problems. Clouditera, by training the SecGPT network security LLM using large datasets of cybersecurity, has created an expert system to help researchers provide advice for various cybersecurity tasks. In the field of penetration testing, NTU’s Gelei Deng applied LLMs to web penetration testing, designing PentestGPT to help with target scanning, vulnerability exploitation, and other processes during penetration; BurpGPT uses LLMs to analyze network traffic and identify vulnerabilities that traditional scanners might miss. In reverse engineering, Gepetto, a plugin for the reverse analysis tool IDA Pro, connects with LLMs to perform semantic understanding of decompiled code. In software security research infrastructure, Tsinghua University Associate Professor Chao Zhang’s team used a machine language model (MLM) trained with a machine instruction corpus, not only obtaining more intuitive and understandable decompiled code with program semantics compared to traditional decompiling solutions but also further assisting in solving issues like vulnerability mining and software similarity analysis. … These tools have undoubtedly boosted the efficiency of various stages in security research. However, when it comes to automating the discovery and patching of unknown vulnerabilities—the most desired goal for security researchers—based on DARKNAVY’s security research experience, current LLMs still face many challenges.\nOn one hand, the context window of LLMs limits their understanding of programs. ChatGPT-4’s context window is 128k tokens, while real-world program code is usually large, and vulnerabilities often span multiple files, creating an extremely long context. Although LLMs with context windows supporting tens of millions of tokens have appeared, when handling super-long contexts, the model may lose focus or even forget parts of the content, making it difficult to locate specific code snippets. On the other hand, LLMs struggle with precise calculations and are prone to hallucinations. Security vulnerabilities often come with stringent trigger conditions, and current LLMs find it difficult to perform precise state reasoning and mathematical calculations, leading to incorrect judgments about program states and false positives. The complexity of static code and the uncertainty of states make it difficult for LLMs to verify the authenticity of vulnerabilities through simple reasoning. In 2024, to push the boundaries of LLMs in automated vulnerability discovery, teams in the AIxCC competition, Google, are trying to use AI agents, combining traditional vulnerability analysis methods, adding more tools to LLMs, and guiding the models to reference human research methods to conduct more autonomous vulnerability analysis.\nNaptime Google’s “Naptime” project, through AI agents, provides a series of tools commonly used by human researchers, enabling the agent to mimic human thinking and behavior. Through iterative vulnerability analysis and hypothesis-driven research, the agent autonomously selects and uses tools to obtain more accurate information, enhancing its ability to find vulnerabilities.\nCode browser: The agent can use the code browser to read specific sections of code in the code repository like a human, ensuring that “Naptime” can focus on analyzing specific functions or variables when handling large codebases. Debugger: Helps the agent obtain information during program execution. The agent can set breakpoints in the debugger and observe the program’s behavior under different input data, enabling dynamic analysis. Python tools: The agent can run Python scripts to precisely calculate the program’s intermediate state. Thus, Naptime can mimic human research methods, browsing code repositories for the code of interest, analyzing them based on function call relationships, and implementing a fully automated vulnerability discovery process.\nIn October 2024, the evolved version of Naptime, Big Sleep, discovered a potential 0-day vulnerability in SQLite.\nDuring code browsing, Big Sleep became interested in an assertion in the vulnerability function, just like a human researcher, and began analyzing the possibility of triggering the assertion and inferring the trigger conditions. Big Sleep then set the input iCol to -1 and used the debugger to test, successfully triggering the assertion that caused a crash.\nAlthough this vulnerability triggered an assertion in the debugging environment, Google researchers found that the release version did not contain this assertion, making the vulnerability exploitable.\nBy making the agent read the code and test inputs like humans, Google successfully leveraged the LLM’s code understanding ability to automate code security analysis and avoid false positives caused by model hallucinations.\nAIxCC In SQLite3, as early as August 2024, the AIxCC organizers reported a vulnerability found by Team Atlanta, an off-by-one error causing null pointer dereferencing.\nWhy didn’t Team Atlanta report the vulnerability themselves?\nBecause the vulnerability was discovered by Team Atlanta’s robot Atlantis in the isolated environment of the AIxCC semifinal competition.\nAIxCC requires the teams’ Cyber Reasoning Systems (CRS) to autonomously discover and patch vulnerabilities. Unlike the 2016 CGC competition, AIxCC requires CRS to be based on large language models and to mine and patch vulnerabilities in real-world application source code written in multiple programming languages.\nTo test CRS’s generalization ability, teams did not know the specific challenge topics before the competition, and all teams could only interact with a general-purpose LLM, using the same computational resources and token limits.\nThe AIxCC semifinals involved several large C-based projects (Linux Kernel, Nginx, SQLite3) and Java-based projects (Tika, Jenkins), with dozens of different types of vulnerabilities manually inserted into the code.\nGiven the large codebases like the Linux Kernel, each project’s token limit was only $100, and teams needed to think about how to use LLMs to identify key code segments.\nTeams combined traditional static and dynamic program analysis methods with LLMs.\nTheori used LLMs to generate fuzz test cases, harnesses, and other methods to help fuzzer increase coverage and assist in vulnerability mining. Team Atlanta used the LLM agent to simulate the thinking of security researchers. Incorporating the “baby-security-AGI” system to distill human researchers’ experiences and practices into structured prompts, allowing Atlantis to replicate security experts’ habits in the code audit process. LLMs were also used as static analysis tools to solve complex problems in traditional program analysis (e.g., pointer analysis, interprocedural analysis) and enable Atlantis to use fuzz testing tools to find triggering code. The use of AI agents in automated vulnerability discovery has demonstrated the ability to mimic human researchers’ process of finding unknown vulnerabilities, as seen in Team Atlanta and Google’s Naptime project, highlighting the immense potential of AI agents.\nAs Sam Altman mentioned, AI agent technology, seen as a potential path to AGI, will have its productivity further unleashed by 2025.\nIn subfields of cybersecurity, integrating LLMs with existing methods shows broad promise:\nAssociate Professor Chao Zhang (Tsinghua University):“While directly using AI models for vulnerability mining is challenging due to scarce vulnerability data and semantic understanding gaps, combining them with existing techniques yields rapid progress.”\nDARKNAVY INSIGHT In 2024, generative AI integrated into more software, enabling richer application scenarios.\nWhether it’s letting the AI on your phone order coffee or write email summaries, or generating short videos with a click, the era of AI agents has arrived.\nAs the brain of AI agents, LLMs are constantly improving their reasoning abilities. At the same time, by integrating human processing methods as the agent’s professional domain knowledge, more human-like work patterns are being realized.\nBy expanding the agent’s toolset to allow access to more diverse information sources and interaction methods, we believe that by 2025, AI agents in the security field will make significant progress in reasoning, generalization, and tool usage.\nThe next generation of AI agents will have excellent reasoning and generalization abilities and be skilled at using a variety of security research tools, inheriting a wealth of human expert knowledge. They will be able to discover more 0-day vulnerabilities in the real world, like top security experts.\nAs we usher in the new era of intelligent AI agents, we must recognize that the double-edged sword of technology has already pierced through the digital barrier. The jailbreak of LLMs will have an impact on the physical world. In the age of intelligent agents, the security of these agents is crucial. Before opening Pandora’s box, we must first be the guardians of the sword.\nReference https://googleprojectzero.blogspot.com/2024/06/project-naptime.html https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html https://sqlite.org/forum/forumpost/171bcc2bcd5e39c3 https://team-atlanta.github.io/blog/post-asc-sqlite/ https://dashboard.aicyberchallenge.com/ascsummary https://blog.theori.io/winning-the-aixcc-qualification-round-7263d1cde9c8 https://blog.trailofbits.com/2024/08/12/trail-of-bits-advances-to-aixcc-finals/ https://shellphish.net/aixcc/index.html https://blog.samaltman.com/reflections https://github.com/Clouditera/secgpt https://github.com/protectai/vulnhuntr https://github.com/hackerai-tech/PentestGPT https://github.com/msoedov/agentic_security https://github.com/aress31/burpgpt https://github.com/JusticeRage/Gepetto https://github.com/albertan017/LLM4Decompile ","wordCount":"1533","inLanguage":"en","image":"https://www.darknavy.org/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/74e16a34-d20c-4282-99af-4f93482ad22e.png","datePublished":"2025-02-10T11:25:50+08:00","dateModified":"2025-02-10T11:25:50+08:00","author":{"@type":"Person","name":"DARKNAVY"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.darknavy.org/darknavy_insight/the_most_imaginative_new_applications_of_2024/"},"publisher":{"@type":"Organization","name":"DARKNAVY","logo":{"@type":"ImageObject","url":"https://www.darknavy.org/images/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://www.darknavy.org/ accesskey=h title="  (Alt + H)"><img src=https://www.darknavy.org/images/darknavy_shenlan_dot.png alt aria-label=logo height=20></a><div class=logo-switches><ul class=lang-switch><li>|</li><li><a href=https://www.darknavy.org/zh/ title=Chinese aria-label=Chinese>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://www.darknavy.org/ title=Home><span>Home</span></a></li><li><a href=https://www.darknavy.org/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://www.darknavy.org/darknavy_insight/ title=Insight><span>Insight</span></a></li><li><a href=https://www.darknavy.org/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.darknavy.org/>Home</a>&nbsp;»&nbsp;<a href=https://www.darknavy.org/darknavy_insight/>DARKNAVY INSIGHT</a></div><h1 class="post-title entry-hint-parent">The Most Imaginative New Applications of 2024</h1><div class=post-meta><span title='2025-02-10 11:25:50 +0800 CST'>February 10, 2025</span>&nbsp;·&nbsp;1533 words&nbsp;·&nbsp;DARKNAVY&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://www.darknavy.org/zh/darknavy_insight/the_most_imaginative_new_applications_of_2024/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#naptime>Naptime</a></li><li><a href=#aixcc>AIxCC</a></li><li><a href=#darknavy-insight>DARKNAVY INSIGHT</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></details></div><div class=post-content><p><img loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/0127cf48-39e2-4864-bafc-4b2ce7a7cef1.png></p><p>2023 was the dawn of generative AI and large language models, which output content in unprecedented ways.</p><p>In 2024, a large number of AI agents emerged, expanding the capabilities of LLM, driving more widespread tool usage, and extending their application to more fields.</p><p>For security researchers, how to leverage AI to improve work efficiency, and even drive AI to think, analyze, and find vulnerabilities like humans, has become a key topic.</p><p>Will we embrace AI first, or will AI replace us? When will this day arrive?</p><p>The following is the third article of the &ldquo;DARKNAVY INSIGHT | 2024 Annual Security Report&rdquo;.</p><p><img loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/74e16a34-d20c-4282-99af-4f93482ad22e.png></p><p>With the emergence of generative AI, its impressive code understanding abilities undoubtedly show its potential to revolutionize the field of security research.</p><p>Since 2023, security researchers have begun trying to use the knowledge base and content generation abilities of LLMs to improve the efficiency of various stages in security research:</p><p>Asking LLMs questions to help security researchers quickly understand the functionality of code, using LLMs to quickly generate test code, integrating LLMs into IDEs to provide coding security suggestions, etc.</p><p><img alt="DARKNAVY, using LLM to assist in analyzing discovered Chrome AI module high-risk vulnerabilities" loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/ff58114b-f449-4be5-a1ec-032e55f03142.png></p><p>We also saw the emergence of a series of LLM-based tools:</p><ul><li>Using LLMs as a security research knowledge base can effectively improve the efficiency of solving unfamiliar domain problems. Clouditera, by training the SecGPT network security LLM using large datasets of cybersecurity, has created an expert system to help researchers provide advice for various cybersecurity tasks.</li><li>In the field of penetration testing, NTU&rsquo;s Gelei Deng applied LLMs to web penetration testing, designing PentestGPT to help with target scanning, vulnerability exploitation, and other processes during penetration; BurpGPT uses LLMs to analyze network traffic and identify vulnerabilities that traditional scanners might miss.</li><li>In reverse engineering, Gepetto, a plugin for the reverse analysis tool IDA Pro, connects with LLMs to perform semantic understanding of decompiled code.</li><li>In software security research infrastructure, Tsinghua University Associate Professor Chao Zhang&rsquo;s team used a machine language model (MLM) trained with a machine instruction corpus, not only obtaining more intuitive and understandable decompiled code with program semantics compared to traditional decompiling solutions but also further assisting in solving issues like vulnerability mining and software similarity analysis.</li><li>…</li></ul><p><img alt="Chao Zhang present MLM at GEEKCON 2024" loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/75375846-6165-4b8e-82f6-402c895bef48.png></p><p>These tools have undoubtedly boosted the efficiency of various stages in security research. However, when it comes to <strong>automating the discovery and patching of unknown vulnerabilities</strong>—the most desired goal for security researchers—based on DARKNAVY&rsquo;s security research experience, current LLMs still face many challenges.</p><ul><li>On one hand, <strong>the context window of LLMs limits their understanding of programs</strong>. ChatGPT-4&rsquo;s context window is 128k tokens, while real-world program code is usually large, and vulnerabilities often span multiple files, creating an extremely long context. Although LLMs with context windows supporting tens of millions of tokens have appeared, when handling super-long contexts, the model may lose focus or even forget parts of the content, making it difficult to locate specific code snippets.</li><li>On the other hand, <strong>LLMs struggle with precise calculations and are prone to hallucinations</strong>. Security vulnerabilities often come with stringent trigger conditions, and current LLMs find it difficult to perform precise state reasoning and mathematical calculations, leading to incorrect judgments about program states and false positives. The complexity of static code and the uncertainty of states make it difficult for LLMs to verify the authenticity of vulnerabilities through simple reasoning.</li></ul><p>In 2024, to push the boundaries of LLMs in automated vulnerability discovery, teams in the AIxCC competition, Google, are trying to use AI agents, combining traditional vulnerability analysis methods, adding more tools to LLMs, and guiding the models to reference human research methods to conduct more autonomous vulnerability analysis.</p><h2 id=naptime>Naptime<a hidden class=anchor aria-hidden=true href=#naptime>#</a></h2><p>Google&rsquo;s &ldquo;Naptime&rdquo; project, through AI agents, provides a series of tools commonly used by human researchers, enabling the agent to mimic human thinking and behavior. Through iterative vulnerability analysis and hypothesis-driven research, the agent autonomously selects and uses tools to obtain more accurate information, enhancing its ability to find vulnerabilities.</p><ul><li><strong>Code browser</strong>: The agent can use the code browser to read specific sections of code in the code repository like a human, ensuring that &ldquo;Naptime&rdquo; can focus on analyzing specific functions or variables when handling large codebases.</li><li><strong>Debugger</strong>: Helps the agent obtain information during program execution. The agent can set breakpoints in the debugger and observe the program&rsquo;s behavior under different input data, enabling dynamic analysis.</li><li><strong>Python tools</strong>: The agent can run Python scripts to precisely calculate the program&rsquo;s intermediate state.</li></ul><p>Thus, Naptime can mimic human research methods, browsing code repositories for the code of interest, analyzing them based on function call relationships, and implementing a fully automated vulnerability discovery process.</p><p><img alt="Naptime Architecture" loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/4b4d2a42-36ea-4836-8a9b-b54be693ad76.png></p><p>In October 2024, the evolved version of Naptime, Big Sleep, discovered a potential 0-day vulnerability in SQLite.</p><p>During code browsing, Big Sleep became interested in an assertion in the vulnerability function, just like a human researcher, and began analyzing the possibility of triggering the assertion and inferring the trigger conditions. Big Sleep then set the input <code>iCol</code> to -1 and used the debugger to test, successfully triggering the assertion that caused a crash.</p><p><img alt="Big Sleep Vulnerability Discovery Process" loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/b54b40b6-01ed-433b-b6c4-566a847eb13d.png></p><p>Although this vulnerability triggered an assertion in the debugging environment, Google researchers found that the release version did not contain this assertion, making the vulnerability exploitable.</p><p><strong>By making the agent read the code and test inputs like humans, Google successfully leveraged the LLM&rsquo;s code understanding ability to automate code security analysis and avoid false positives caused by model hallucinations.</strong></p><h2 id=aixcc>AIxCC<a hidden class=anchor aria-hidden=true href=#aixcc>#</a></h2><p>In SQLite3, as early as August 2024, the AIxCC organizers reported a vulnerability found by Team Atlanta, an off-by-one error causing null pointer dereferencing.</p><p><img alt="SQLite3 null pointer dereference report" loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/c6db1e97-815d-4240-923a-8c01f07862a4.png></p><p><strong>Why didn&rsquo;t Team Atlanta report the vulnerability themselves?</strong></p><p>Because the vulnerability was discovered by Team Atlanta&rsquo;s robot Atlantis in the isolated environment of the AIxCC semifinal competition.</p><blockquote><p>AIxCC requires the teams&rsquo; Cyber Reasoning Systems (CRS) to autonomously discover and patch vulnerabilities. Unlike the 2016 CGC competition, AIxCC requires CRS to be based on large language models and to mine and patch vulnerabilities in real-world application source code written in multiple programming languages.</p><p>To test CRS&rsquo;s generalization ability, teams did not know the specific challenge topics before the competition, and all teams could only interact with a general-purpose LLM, using the same computational resources and token limits.</p><p>The AIxCC semifinals involved several large C-based projects (Linux Kernel, Nginx, SQLite3) and Java-based projects (Tika, Jenkins), with dozens of different types of vulnerabilities manually inserted into the code.</p></blockquote><p><img alt="AIXCC Semifinal Competition" loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/c759760d-d828-4611-9cd4-14865757d39c.png></p><p>Given the large codebases like the Linux Kernel, each project&rsquo;s token limit was only $100, and teams needed to think about how to use LLMs to identify key code segments.</p><p>Teams combined traditional static and dynamic program analysis methods with LLMs.</p><ul><li>Theori used LLMs to generate fuzz test cases, harnesses, and other methods to help fuzzer increase coverage and assist in vulnerability mining.</li><li>Team Atlanta used the LLM agent to simulate the thinking of security researchers.<ul><li>Incorporating the &ldquo;baby-security-AGI&rdquo; system to distill human researchers&rsquo; experiences and practices into structured prompts, allowing Atlantis to replicate security experts&rsquo; habits in the code audit process.</li><li>LLMs were also used as static analysis tools to solve complex problems in traditional program analysis (e.g., pointer analysis, interprocedural analysis) and enable Atlantis to use fuzz testing tools to find triggering code.</li></ul></li></ul><p><img alt="Cyber Reasoning Systems (CRS) of Team Atlanta: Atlantis" loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/668a551e-40a5-4f1d-bc4b-fafb58a4b281.png></p><p>The use of AI agents in automated vulnerability discovery has demonstrated the ability to mimic human researchers&rsquo; process of finding unknown vulnerabilities, as seen in Team Atlanta and Google&rsquo;s Naptime project, highlighting the immense potential of AI agents.</p><p>As Sam Altman mentioned, AI agent technology, seen as a potential path to AGI, will have its productivity further unleashed by 2025.</p><p><img alt="Sam Altman&rsquo;s blog on January 6th" loading=lazy src=/darknavy_insight/the_most_imaginative_new_applications_of_2024/attachments/8a61207e-255a-456e-a21f-91ef44a2e815.png></p><p>In subfields of cybersecurity, integrating LLMs with existing methods shows broad promise:</p><blockquote><p><strong>Associate Professor Chao Zhang (Tsinghua University)</strong>:<em>&ldquo;While directly using AI models for vulnerability mining is challenging due to scarce vulnerability data and semantic understanding gaps, combining them with existing techniques yields rapid progress.&rdquo;</em></p></blockquote><hr><h2 id=darknavy-insight>DARKNAVY INSIGHT<a hidden class=anchor aria-hidden=true href=#darknavy-insight>#</a></h2><p>In 2024, generative AI integrated into more software, enabling richer application scenarios.</p><p>Whether it&rsquo;s letting the AI on your phone order coffee or write email summaries, or generating short videos with a click, the era of AI agents has arrived.</p><p>As the brain of AI agents, LLMs are constantly improving their reasoning abilities. At the same time, by integrating human processing methods as the agent&rsquo;s professional domain knowledge, more human-like work patterns are being realized.</p><p>By expanding the agent&rsquo;s toolset to allow access to more diverse information sources and interaction methods, we believe that by 2025, AI agents in the security field will make significant progress in reasoning, generalization, and tool usage.</p><p>The next generation of AI agents will have excellent reasoning and generalization abilities and be skilled at using a variety of security research tools, inheriting a wealth of human expert knowledge. They will be able to discover more 0-day vulnerabilities in the real world, like top security experts.</p><p>As we usher in the new era of intelligent AI agents, we must recognize that the double-edged sword of technology has already pierced through the digital barrier. The jailbreak of LLMs will have an impact on the physical world. In the age of intelligent agents, the security of these agents is crucial. Before opening Pandora&rsquo;s box, we must first be the guardians of the sword.</p><hr><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li><a href=https://googleprojectzero.blogspot.com/2024/06/project-naptime.html>https://googleprojectzero.blogspot.com/2024/06/project-naptime.html</a></li><li><a href=https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html>https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html</a></li><li><a href=https://sqlite.org/forum/forumpost/171bcc2bcd5e39c3>https://sqlite.org/forum/forumpost/171bcc2bcd5e39c3</a></li><li><a href=https://team-atlanta.github.io/blog/post-asc-sqlite/>https://team-atlanta.github.io/blog/post-asc-sqlite/</a></li><li><a href=https://dashboard.aicyberchallenge.com/ascsummary>https://dashboard.aicyberchallenge.com/ascsummary</a></li><li><a href=https://blog.theori.io/winning-the-aixcc-qualification-round-7263d1cde9c8>https://blog.theori.io/winning-the-aixcc-qualification-round-7263d1cde9c8</a></li><li><a href=https://blog.trailofbits.com/2024/08/12/trail-of-bits-advances-to-aixcc-finals/>https://blog.trailofbits.com/2024/08/12/trail-of-bits-advances-to-aixcc-finals/</a></li><li><a href=https://shellphish.net/aixcc/index.html>https://shellphish.net/aixcc/index.html</a></li><li><a href=https://blog.samaltman.com/reflections>https://blog.samaltman.com/reflections</a></li><li><a href=https://github.com/Clouditera/secgpt>https://github.com/Clouditera/secgpt</a></li><li><a href=https://github.com/protectai/vulnhuntr>https://github.com/protectai/vulnhuntr</a></li><li><a href=https://github.com/hackerai-tech/PentestGPT>https://github.com/hackerai-tech/PentestGPT</a></li><li><a href=https://github.com/msoedov/agentic_security>https://github.com/msoedov/agentic_security</a></li><li><a href=https://github.com/aress31/burpgpt>https://github.com/aress31/burpgpt</a></li><li><a href=https://github.com/JusticeRage/Gepetto>https://github.com/JusticeRage/Gepetto</a></li><li><a href=https://github.com/albertan017/LLM4Decompile>https://github.com/albertan017/LLM4Decompile</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.darknavy.org/>DARKNAVY</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>